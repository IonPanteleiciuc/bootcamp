{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#023F7C> **Machine Learning, Explainability and Deep Learning** </font>\n",
    "\n",
    "<font color=#023F7C>**Hi! PARIS DataBootcamp 2023 üöÄ**</font> <br>\n",
    "\n",
    "\n",
    "<img src = https://www.hi-paris.fr/wp-content/uploads/2020/09/logo-hi-paris-retina.png width = \"300\" height = \"200\" >\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before you start to working on this notebook ‚ö†Ô∏è**: <br>\n",
    "Please download/copy this notebook from `hfactory_magic_folders\\course` and drop it into your own directory `my_work` on HFactory. <br>\n",
    "If you don't, you won't be able to save the modifications you've made on this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to work with this notebook ? üìù** <br>\n",
    "Here are some guidelines on how you should work on this notebook during the week. <br>\n",
    "*You don't need to finish the whole notebook before sending it to us on Friday*\n",
    "- Wednesday: Work on section 1. and 2. (Import dataset and Machine Learning)\n",
    "- Thursday: Finish section 2., work on section 3. and 4. (ML, Explainability, Deep Learning)\n",
    "- Friday morning: Finish the notebook as best you can for the final deliverable at 12:00pm \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bootcamp deliverables** üíØ: <br>\n",
    "Send us the completed notebook before 12:00pm (midi) on Friday at `data-event@hi-paris.fr`<br>\n",
    "*Don't forget to also send us the powerpoint deliverable on Friday*\n",
    "- Send a single notebook per group. \n",
    "- Add the names of the members of your group in your email submission \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Need help ? üôè** <br>\n",
    "You can find code examples in this morning's <b>Machine Learning</b> and <b>Optimisation and model evaluation</b> theortical courses. <br>\n",
    "If you are really struggling with this section, you can also visit the `Data_Science_crash_course.ipynb` notebook in the Pre-bootcamp folder on HFactory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Import libraries and clean dataset**\n",
    "\n",
    "**Let's start by importing the libraries we used in two previous notebooks.**\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZyhKqCkh5D8S",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import time\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None) #Show all columns\n",
    "\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, let's import `scikit-learn` functions for classification models, data preprocessing and performance metrics.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZS8yjKdl5dGt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Classification models\n",
    "from sklearn.linear_model import LogisticRegression,LinearRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Multiclass classification \n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "\n",
    "# Preprocessing tools\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder,StandardScaler\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Performance metrics\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve, classification_report, explained_variance_score\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "# Improve your model\n",
    "# Improve your model\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, import the `dataset_train_clean.csv` dataset you cleaned/worked on in `Data_Clean.ipynb` and `Dataviz.ipynb`** <br>\n",
    "*Make sure you name the loaded dataframe `dataset` in the notebook !*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you weren't able to save/create `dataset_train_clean.csv` then run the following code.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path=r'~/hfactory_magic_folders/course/Dataset/dataset_train.csv'\n",
    "#path=r\"dataset_train.csv\"\n",
    "\n",
    "# Import the csv file\n",
    "dataset = pd.read_csv(path,encoding='latin-1',sep=';')\n",
    "\n",
    "# Clean the dataframe \n",
    "to_drop=['Unnamed: 0','Customer Email','Customer Fname','Customer Lname','Customer Password','Customer Street','Order Zipcode','Product Description', \"Sales per customer\",\"Product Card Id\"]\n",
    "dataset=dataset.drop(to_drop,axis=1)\n",
    "dataset=dataset.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can drop more columns at this step if you don't think they will be useful in the Machine Learning model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Machine Learning**\n",
    "\n",
    "**You can chose either `Late_delivery_risk` or `Delivery_status` as the variable to predict.**\n",
    "- Predict `Late_delivery_risk` to try binary classification (Beginner level)\n",
    "- Predict `Delivery_Status` to try multi-class classification (Intermediate/Advanced level)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Important information ‚ö†Ô∏è**: <br>\n",
    "If you pick `Late_delivery_risk` then `Delivery_status` should be deleted from the dataset (and vice-versa). <br>\n",
    "If you need help doing this, the next cell will do it for you.\n",
    "- Keep `binary_classification=True` if you pick binary classification.\n",
    "- Change it to `binary_classification=False` if you pick multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose your figther\n",
    "binary_classification=True\n",
    "\n",
    "if binary_classification is True:\n",
    "    dataset=dataset.drop(columns=['Delivery Status'])\n",
    "    name_label=['Late_delivery_risk']\n",
    "else:\n",
    "    dataset=dataset.drop(columns=['Late_delivery_risk'])\n",
    "    name_label=['Delivery Status_Advance shipping', 'Delivery Status_Late delivery','Delivery Status_Shipping canceled', 'Delivery Status_Shipping on time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1 Data Preprocessing**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** <br>**Transform the categorical variables (with less than 15 unique values) into numerical variables with OneHotEncoding (OHE).** <br>\n",
    "*Make sure you don't include `Late_delivery_risk` with the variables to transform with OneHotencoding (if you are going to do binary classification)* <br>\n",
    "*You can go this [page](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) for more info on how to use scikit-learn's `OneHotEncoder` function.*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You can run the next cell if you are struggling to select the correct variables to Onehotencode.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0lEUko-z6u4i",
    "outputId": "fac7e6e0-f37e-46da-e1a3-3d8d177df6bc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Run this following code if you need help selecting variables to onehotencode\n",
    "\n",
    "df_continuous=dataset.select_dtypes(include=[\"float64\"])\n",
    "df_categorical=dataset.select_dtypes(include=[\"object\",\"int64\"])\n",
    "nb_unique_value_max=15\n",
    "\n",
    "# List of columns to transform with OneHotEncoding\n",
    "to_OHE=[key for key in df_categorical.keys() if len(df_categorical[key].drop_duplicates())<nb_unique_value_max] \n",
    "\n",
    "# Remove Late_delivery_risk or Delivery Status from list of columns to transform with OHE\n",
    "if \"Late_delivery_risk\" in to_OHE:\n",
    "    to_OHE.remove(\"Late_delivery_risk\")\n",
    "    \n",
    "if \"Delivery Status\" in to_OHE:\n",
    "    to_OHE.remove(\"Delivery Status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concatenate the OneHotencoded variables/dataframe with the original dataframe using `pd.concat([...],axis=1)`.<br>**\n",
    "*Make sure to drop the columns you transformed with OneHotEncoding in the concatenated dataframe*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** <br>\n",
    "**Transform the remaining categorical variables (those with more than 15 unique values) using `LabelEncoder`**. <br>\n",
    "If you need help selecting categorical variables with more than 15 unique values, run the following cell\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: The `LabelEncoder` function can only transform 1 column at a time, where as `OneHotEncoder` can directly transform multiple columns.* <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3 (Bonus)**: <br>\n",
    "**Try other data preprocessing methods on the data (StandardScaler, MinMaxScaler...).**<br>\n",
    "*Scale continuous feature variables (with a float type), not categorical*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**: <br>\n",
    "**Create a `y` variable with the target variable in the dataset you've chosen to predict.** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a `X` variable with the remaining feature variables of your dataset.** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XhryaZlk7c7R"
   },
   "source": [
    "\n",
    "**Question 5**: <br>\n",
    "**Split X and y into training and validation/test sets using scikit-learn's `train_test_split()` function. <br>**\n",
    "\n",
    "*Note: Add `stratify=y` to make sure your splits are stratified and `random_state=42`*\n",
    "- *The training set will be used to train/fit our data to the model*\n",
    "- *The test/validation set will be used to quantify the performance of the model on new data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zg8ZDc7E5LD3",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9P9cvpH73he"
   },
   "source": [
    "### **2.2 Train/fit a Machine Learning model**\n",
    "Now that our dataset is clean and has the right format, we can use it train Machine Learning models with this data.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6**: <br>\n",
    "**Train Logistic Regression, DecisionTree and Random forest models using scikit-learn's `.fit()` method. <br>**\n",
    "- Logistic Regression in scikit-learn: `LogisticRegression()` \n",
    "- Decision Tree in scikit-learn: `DecisionTreeClassifier()`\n",
    "- Random Forest in scikit-learn: `RandomForestClassifier()`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "id": "v3DAahos6fY7",
    "outputId": "b3f1fcbc-e9bd-44a4-b3a3-23c05ab1485e",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7**: <br>\n",
    "**Compute the probability of the predicted values for the trained models with `.predict_proba()`**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8 (Bonus)**: <br>\n",
    "**Now do the same (train and predict proba) with two other classification models.** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3 Evaluate/test the performance of the models**\n",
    "\n",
    "**Question 9**: <br>\n",
    "**Compute the AUC score of the trained models with `roc_auc_score()`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If the AUC score of your models are low, here are some tips/ideas of how you can improve their performances:**\n",
    "- **You can retrain the models in Question 6 and 7 with different parameters.** <br>\n",
    "- **You can drop columns or include more in the dataset**\n",
    "- **You can scale the dataset with `StandardScaler` or `MinMaxScaler`**\n",
    "- **You can try to find a model's optimal parameters with `GridSearch`, `RandomSearch`, `crossvalscore` (Section 2.4)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPA8yhL-Vr0h"
   },
   "source": [
    "**Question 10**: <br>\n",
    "**Display the ROC curve of the models you've trained with `roc_curve()` and add their AUC score on the plot.** <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "zYDOE8Dvdl2I",
    "outputId": "e7805434-fc8e-453b-ef18-2fa2179ed827",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQuofktDkbwM"
   },
   "source": [
    "**Question 11**: <br>\n",
    "**Create a confusion matrix for the two models with the best AUC score.** <br>\n",
    "**Then, use `sns.heatmap()` to plot the confusion matrix**. <br>\n",
    "\n",
    "*You need to compute the prediction of the model with `.predict()` to compute the confusion matrix*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 481
    },
    "id": "arkp_xaxkvGv",
    "outputId": "18d359c9-6227-4237-c6c8-9d15478e04ff",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytj5AGeWI13g"
   },
   "source": [
    "### **2.4 Upgrade your model! (Bonus)**\n",
    "\n",
    "**Use Grid Search, Cross validation and/or Random Search to find the optimal parameters for your model(s).** <br>\n",
    "*Don't use these optimization methods on every model you've tested, just try them on the best model you've tested.* <br>\n",
    "*For `GridSearchCV`, use the parameter `scoring='roc_auc'` to use the AUC score as the performance metric.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Explainability with shap**\n",
    "\n",
    "The `shap` library (SHapley Additive exPlanations) is a Python library used for explaining the output of machine learning models. <br> It provides a unified framework for interpreting complex models and understanding the contributions of individual features to model predictions. <br> \n",
    "\n",
    "Shap is particularly useful for understanding black-box models like boosting, random forests, and deep neural networks, among others. <br>\n",
    "It can also be used with any classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's install and import the shap library.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shap in /opt/conda/lib/python3.10/site-packages (0.41.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from shap) (1.24.4)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from shap) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from shap) (1.2.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from shap) (2.0.3)\n",
      "Requirement already satisfied: tqdm>4.25.0 in /opt/conda/lib/python3.10/site-packages (from shap) (4.65.0)\n",
      "Requirement already satisfied: packaging>20.9 in /opt/conda/lib/python3.10/site-packages (from shap) (23.1)\n",
      "Requirement already satisfied: slicer==0.0.7 in /opt/conda/lib/python3.10/site-packages (from shap) (0.0.7)\n",
      "Requirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (from shap) (0.57.1)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from shap) (2.2.1)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba->shap) (0.40.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->shap) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->shap) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->shap) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->shap) (1.3.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->shap) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "np.bool=bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shap is very heavy and takes a long time to compute. <br> \n",
    "To facilitate execution and reduce computing time, we'll work on the **first 1000 rows** of X_train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_shap = X_train[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 10**: <br>\n",
    "**Create an object that can compute the shap values using `shap.Explainer`** <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, compute the shap values of a trained model with `.shap_values()`.** <br>\n",
    "Save the computed shap values in a `shap_values` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 11**: <br>\n",
    "**Create multiple shap summary plots using `shap.summary_plot()`**:\n",
    "- Create the first plot with `shap_values` \n",
    "- Create the second plot with `shap_values[0]` and `plot_type=\"dot\"` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 12 (Bonus)**: <br>\n",
    "**Display other shap plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Deep Learning (Bonus)**\n",
    "\n",
    "**We will start by importing one of Python's Deep Learning libraries `tensorflow`/`keras`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now should run the following cells to prepare the data to train a Deep Learning model.** <br>\n",
    "*`dataset` should be the dataframe you transformed with data pre-processing (Onehotencoded, LabelEncoder,...).* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_DL = dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Order_Item_Profit_Ratio',\n",
       " 'Sales',\n",
       " 'Order_Item_Total',\n",
       " 'Order_Profit_Per_Order',\n",
       " 'Product_Price']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names = list(dataset_DL.keys())\n",
    "col_names_replace = [sub.replace(\" \", \"_\") for sub in col_names]\n",
    "col_names_replace = [sub.replace(\"(\", \"_\") for sub in col_names_replace]\n",
    "col_names_replace = [sub.replace(\")\", \"\") for sub in col_names_replace]\n",
    "col_names_replace[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dict_map = {}\n",
    "for key,value in zip(col_names, col_names_replace):\n",
    "  dict_map[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_DL.columns = dataset_DL.columns.map(dict_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86653 train examples\n",
      "21664 validation examples\n",
      "27080 test examples\n"
     ]
    }
   ],
   "source": [
    "name_label='Late_delivery_risk'\n",
    "train, test = train_test_split(dataset_DL, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.2)\n",
    "print(len(train), 'train examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')\n",
    "X_dataset=dataset_DL.drop([name_label],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop(name_label)\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "# numeric cols\n",
    "for header in X_dataset.keys():\n",
    "  feature_columns.append(feature_column.numeric_column(header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(dataset_DL, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32 # A small batch sized is used for demonstration purposes\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 14** <br>\n",
    "**Make a small neural network model using `tensorflow`/`keras`, and print the accuracy**\n",
    "\n",
    "*Note: You can use the following elements to train the neural network* <br>\n",
    "- *`tf.keras.Sequential`*\n",
    "- *`layers.Dense(INTEGER, activation='relu')`*,\n",
    "- *`tf.keras.losses.BinaryCrossentropy`*\n",
    "- *`model.compile(optimizer='adam', ...)`* \n",
    "- *`model.fit`*\n",
    "- *`model.evaluate`* with epoch ~= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
